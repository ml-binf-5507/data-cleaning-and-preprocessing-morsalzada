{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e65467b",
   "metadata": {},
   "source": [
    "# Data Preprocessing Tutorial\n",
    "\n",
    "This notebook walks through the key steps of a data preprocessing pipeline:\n",
    "\n",
    "1. **Data Cleaning** - Handle missing values and duplicates\n",
    "2. **Exploratory Analysis** - Understand your data types\n",
    "3. **Feature Engineering** - Encode categorical variables\n",
    "4. **Normalization** - Scale numeric features\n",
    "5. **Train/Test Split** - Avoid data leakage\n",
    "\n",
    "We'll use a sample patient dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dcafd",
   "metadata": {},
   "source": [
    "## Step 0: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c47606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c033f",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399b84b",
   "metadata": {},
   "source": [
    "## Step 2: Clean Missing Values\n",
    "\n",
    "### Understanding the Problem\n",
    "Notice that missing values are represented in different ways:\n",
    "- `NA` (string)\n",
    "- `N/A` (string)\n",
    "- `NaN` (pandas null)\n",
    "\n",
    "Pandas only recognizes `NaN` as missing. We need to replace the string versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE: Show missing values\n",
    "print(\"BEFORE replacing:\")\n",
    "print(f\"Age column (raw): {df['age'].head(10).tolist()}\")\n",
    "print(f\"City column (raw): {df['city'].head(10).tolist()}\")\n",
    "print(f\"\\nMissing count: {df[['age', 'income', 'city']].isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Replace missing value strings with NaN\n",
    "df = df.replace([\"NA\", \"N/A\", \"na\", \"n/a\", \"NaN\", \"nan\", \"\"], np.nan)\n",
    "\n",
    "print(\"AFTER replacing strings with NaN:\")\n",
    "print(f\"Age column: {df['age'].head(10).tolist()}\")\n",
    "print(f\"City column: {df['city'].head(10).tolist()}\")\n",
    "print(f\"\\nMissing count by column:\")\n",
    "print(df[['age', 'income', 'city']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Impute missing numeric values with median\n",
    "# Why median? It's robust to outliers\n",
    "\n",
    "print(f\"Age median: {df['age'].median()}\")\n",
    "print(f\"Income median: {df['income'].median()}\")\n",
    "\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['income'] = df['income'].fillna(df['income'].median())\n",
    "\n",
    "print(\"\\nAFTER imputing numeric columns:\")\n",
    "print(df[['age', 'income']].isna().sum())\n",
    "print(f\"\\nAge column (now complete): {df['age'].head(10).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18855b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Impute missing categorical values with mode (most frequent)\n",
    "print(f\"City value counts:\")\n",
    "print(df['city'].value_counts())\n",
    "print(f\"\\nCity mode: {df['city'].mode()[0]}\")\n",
    "\n",
    "df['city'] = df['city'].fillna(df['city'].mode()[0])\n",
    "\n",
    "print(\"\\nAFTER imputing city:\")\n",
    "print(f\"Missing in city: {df['city'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle education (do the same thing)\n",
    "df['education'] = df['education'].fillna(df['education'].mode()[0])\n",
    "\n",
    "print(\"Final missing value check:\")\n",
    "print(df.isna().sum())\n",
    "print(\"\\n✓ All missing values handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9590e40",
   "metadata": {},
   "source": [
    "## Step 3: Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Dataset shape before: {df.shape}\")\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "duplicates_removed = df.duplicated().sum()\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset shape after: {df.shape}\")\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71614061",
   "metadata": {},
   "source": [
    "## Step 4: Identify Feature Types\n",
    "\n",
    "Pandas has different data types:\n",
    "- **Numeric**: `int64`, `float64` (numbers)\n",
    "- **Categorical**: `object` (strings/categories)\n",
    "\n",
    "We need to handle each type differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c28f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns are ID, target, and features\n",
    "id_cols = ['patient_id']\n",
    "target = 'target'\n",
    "\n",
    "# Get all feature columns\n",
    "feature_cols = [c for c in df.columns if c not in id_cols and c != target]\n",
    "\n",
    "# Split into categorical and numeric\n",
    "cat_cols = [c for c in feature_cols if df[c].dtype == 'object']\n",
    "num_cols = [c for c in feature_cols if df[c].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"ID columns: {id_cols}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"\\nNumeric features: {num_cols}\")\n",
    "print(f\"Categorical features: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dd3db",
   "metadata": {},
   "source": [
    "## Step 5: Encode Categorical Features\n",
    "\n",
    "Machine learning models need numbers, not text. **One-hot encoding** converts categories into binary columns.\n",
    "\n",
    "Example:\n",
    "- `city = 'Toronto'` → `city_Toronto=1, city_Vancouver=0, city_Montreal=0`\n",
    "- `city = 'Vancouver'` → `city_Toronto=0, city_Vancouver=1, city_Montreal=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the original categorical data\n",
    "print(\"BEFORE encoding:\")\n",
    "print(df[['city', 'education']].head(10))\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "encoded_columns = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    # Get one-hot encoded version\n",
    "    encoded = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    \n",
    "    # Track column names\n",
    "    encoded_columns.extend(encoded.columns.tolist())\n",
    "    \n",
    "    # Drop original and add encoded\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = pd.concat([df, encoded], axis=1)\n",
    "\n",
    "print(\"AFTER encoding:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nNew shape: {df.shape}\")\n",
    "print(f\"\\nEncoded columns: {encoded_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dd307",
   "metadata": {},
   "source": [
    "## Step 6: Scale Numeric Features\n",
    "\n",
    "**Standardization** converts numeric features to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "Formula: `(x - mean) / std`\n",
    "\n",
    "Why? Many ML algorithms perform better with normalized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show numeric data BEFORE scaling\n",
    "print(\"BEFORE scaling:\")\n",
    "print(df[num_cols].describe())\n",
    "print(f\"\\nAge - mean: {df['age'].mean():.1f}, std: {df['age'].std():.1f}\")\n",
    "print(f\"Income - mean: {df['income'].mean():.0f}, std: {df['income'].std():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! Compute scaling parameters from the FULL data\n",
    "# (In real ML, you'd compute from TRAIN only)\n",
    "\n",
    "means = {}\n",
    "stds = {}\n",
    "\n",
    "for col in num_cols:\n",
    "    means[col] = df[col].mean()\n",
    "    stds[col] = df[col].std()\n",
    "    \n",
    "    # Standardize: (x - mean) / std\n",
    "    df[col] = (df[col] - means[col]) / stds[col]\n",
    "\n",
    "print(\"AFTER scaling:\")\n",
    "print(df[num_cols].describe())\n",
    "print(f\"\\nAge - mean: {df['age'].mean():.6f}, std: {df['age'].std():.6f}\")\n",
    "print(f\"Income - mean: {df['income'].mean():.6f}, std: {df['income'].std():.6f}\")\n",
    "print(\"\\n✓ Now mean ≈ 0 and std ≈ 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195204bb",
   "metadata": {},
   "source": [
    "## Step 7: Train/Test Split (No Data Leakage!)\n",
    "\n",
    "**Data leakage** happens when preprocessing parameters (like scaling) are computed using test data.\n",
    "\n",
    "**Correct approach:**\n",
    "1. Split data into train/test\n",
    "2. Compute scaling parameters from TRAIN\n",
    "3. Apply those parameters to TEST\n",
    "\n",
    "**Wrong approach:**\n",
    "1. Scale using all data\n",
    "2. Split into train/test (test set was used in scaling!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this tutorial, let's start fresh to show the RIGHT way\n",
    "# Reload and re-preprocess\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "# Quick cleaning\n",
    "df = df.replace([\"NA\", \"N/A\", \"na\", \"n/a\", \"NaN\", \"nan\", \"\"], np.nan)\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['income'] = df['income'].fillna(df['income'].median())\n",
    "df['city'] = df['city'].fillna(df['city'].mode()[0])\n",
    "df['education'] = df['education'].fillna(df['education'].mode()[0])\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Data prepared. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-HOT ENCODE (before split, ok)\n",
    "# Note: We're doing this for simplicity. In practice, you'd fit encoders on train only.\n",
    "for col in ['city', 'education']:\n",
    "    encoded = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = pd.concat([df, encoded], axis=1)\n",
    "\n",
    "print(f\"After encoding. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becac4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT INTO TRAIN/TEST\n",
    "# Separate features and target\n",
    "X = df.drop(['patient_id', 'target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Keep same class distribution in train/test\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE NUMERIC FEATURES (from TRAIN parameters only!)\n",
    "\n",
    "# Get numeric columns\n",
    "num_cols = ['age', 'income']\n",
    "\n",
    "# Compute means and stds from TRAIN SET ONLY\n",
    "train_means = {}\n",
    "train_stds = {}\n",
    "\n",
    "for col in num_cols:\n",
    "    train_means[col] = X_train[col].mean()\n",
    "    train_stds[col] = X_train[col].std()\n",
    "    \n",
    "    print(f\"{col} - Train mean: {train_means[col]:.1f}, std: {train_stds[col]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0183ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling using TRAIN statistics to BOTH train and test\n",
    "\n",
    "# Scale TRAIN\n",
    "for col in num_cols:\n",
    "    X_train[col] = (X_train[col] - train_means[col]) / train_stds[col]\n",
    "\n",
    "# Scale TEST using same parameters\n",
    "for col in num_cols:\n",
    "    X_test[col] = (X_test[col] - train_means[col]) / train_stds[col]\n",
    "\n",
    "print(\"TRAIN scaled:\")\n",
    "print(X_train[num_cols].describe())\n",
    "\n",
    "print(\"\\nTEST scaled (using TRAIN parameters):\")\n",
    "print(X_test[num_cols].describe())\n",
    "\n",
    "print(\"\\n✓ Train is standardized (mean≈0, std≈1)\")\n",
    "print(\"✓ Test is scaled with same parameters (no leakage!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd6613",
   "metadata": {},
   "source": [
    "## Step 8: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✓ Cleaned missing values (replaced NA/N/A strings, imputed medians)\")\n",
    "print(f\"✓ Removed {8} duplicate rows\")  # Example\n",
    "print(f\"✓ One-hot encoded: city, education\")\n",
    "print(f\"✓ Scaled numeric: age, income (using TRAIN parameters only)\")\n",
    "print(f\"✓ Split into train/test with stratification (no data leakage)\")\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain set statistics (scaled):\")\n",
    "print(f\"  Age - mean: {X_train['age'].mean():.6f}, std: {X_train['age'].std():.6f}\")\n",
    "print(f\"  Income - mean: {X_train['income'].mean():.6f}, std: {X_train['income'].std():.6f}\")\n",
    "\n",
    "print(f\"\\nTest set statistics (using TRAIN parameters):\")\n",
    "print(f\"  Age - mean: {X_test['age'].mean():.3f}, std: {X_test['age'].std():.3f}\")\n",
    "print(f\"  Income - mean: {X_test['income'].mean():.1f}, std: {X_test['income'].std():.3f}\")\n",
    "print(f\"  (Note: Test means/stds won't be exactly 0/1 - that's expected!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc77687",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Replace missing value strings first** - `NA`, `N/A`, etc. aren't recognized as NaN\n",
    "2. **Impute numeric with median** - Robust to outliers\n",
    "3. **Impute categorical with mode** - Most frequent value\n",
    "4. **One-hot encode categories** - Converts text to numbers ML models need\n",
    "5. **Scale from TRAIN only** - Prevents data leakage\n",
    "6. **Use same parameters for TEST** - Apply train scaling/encoding to test\n",
    "7. **Stratify the split** - Keep class distributions balanced\n",
    "\n",
    "Your assignment asks you to implement these steps in `src/preprocess.py`!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
